{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1eIb3ho6lfVoHhZeG-Cdv4IQNyJERxXx8","authorship_tag":"ABX9TyMpAqetQr5XNliAc89YbAZG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#pre_train.py\n","#ec2-train\n","\n","from transformers import AutoModelForSequenceClassification\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import pandas as pd\n","from transformers import EvalPrediction\n","from sklearn.metrics import accuracy_score\n","from transformers import TrainerCallback\n","\n","\n","# GPU 사용 가능 확인 및 device 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# SentimentDataset 클래스 정의\n","class SentimentDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# CSV 파일 로드\n","import pandas as pd\n","\n","\n","df = pd.read_csv('/home/ec2-user/train_org.csv')  # 'your_dataset.csv'를 실제 파일 경로로 변경하세요.\n","\n","\n","# 데이터 전처리\n","texts = df['document'].tolist()  # 리뷰 텍스트\n","labels = df['label'].tolist()  # 라벨 데이터는 정수로 변환되어 있어야 합니다..\n","labels = [int(label) for label in labels]  # 라벨을 정수형으로 변환\n","texts = [str(text) for text in texts if text is not None]\n","\n","# 토크나이저 로딩\n","tokenizer_roberta = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n","tokenizer_electra = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n","\n","# 데이터셋 토큰화\n","encodings_roberta = tokenizer_roberta(texts, truncation=True, padding=True, max_length=128)\n","encodings_electra = tokenizer_electra(texts, truncation=True, padding=True, max_length=128)\n","\n","# 데이터셋 생성\n","dataset_roberta = SentimentDataset(encodings_roberta, labels)\n","dataset_electra = SentimentDataset(encodings_electra, labels)\n","\n","# 로드할 RoBERTa 모델 경로\n","roberta_model_path = \"/home/ec2-user/roberta_model\"\n","# 로드할 Electra 모델 경로\n","electra_model_path = \"/home/ec2-user/electra_model\"\n","\n","# 저장된 모델 로드\n","model_roberta = AutoModelForSequenceClassification.from_pretrained(roberta_model_path)\n","model_electra = AutoModelForSequenceClassification.from_pretrained(electra_model_path)\n","\n","\n","# 사용자 지정 콜백 클래스 정의\n","class PrintProgressCallback(TrainerCallback):\n","    def on_epoch_end(self, args, state, control, **kwargs):\n","        print(f\"Epoch {state.epoch} 종료. 진행 상태: {state.global_step}/{state.max_steps} ({(state.global_step/state.max_steps)*100:.2f}%)\")\n","        if 'eval_metrics' in kwargs:\n","            print(f\"Epoch {state.epoch} Accuracy: {kwargs['eval_metrics']['eval_accuracy']:.4f}\")\n","\n","# 위에서 정의한 compute_metrics 함수 사용\n","def compute_metrics(p: EvalPrediction):\n","    preds = np.argmax(p.predictions, axis=1)\n","    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n","\n","\n","class CustomTrainer(Trainer):\n","    def __init__(self, label_weights, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.label_weights = label_weights\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # 모델의 출력에서 로스 계산\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","        # 가중치 적용하여 로스 계산\n","        loss_fct = torch.nn.CrossEntropyLoss(weight=self.label_weights)\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss\n","\n","   # def on_step_end(self, args, state, control, **kwargs):\n","   #     super().on_step_end(args, state, control, **kwargs)\n","   #     # 매 1000 스텝마다 모델 저장\n","   #     if state.global_step % 10 == 0:\n","   #         output_dir1 = f\"{args.output_dir1}/checkpoint-{state.global_step}\"\n","   #         self.save_model(output_dir1)\n","   #         print(f\"모델을 {output_dir1}에 저장했습니다.\")\n","   #     if state.global_step % 10 == 0:\n","   #         output_dir2 = f\"{args.output_dir2}/checkpoint-{state.global_step}\"\n","   #         self.save_model(output_dir2)\n","   #         print(f\"모델을 {output_dir2}에 저장했습니다.\")\n","\n","\n","\n","# TrainingArguments 설정\n","training_args = TrainingArguments(\n","    output_dir= './result',\n","    num_train_epochs=1,\n","    per_device_train_batch_size=32,\n","    warmup_steps=160000000,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=800,\n",")\n","\n","\n","# 라벨 가중치 설정\n","# 예시: 각 라벨에 대한 가중치가 동일하다고 가정\n","label_weights = torch.tensor([1.0, 1.0, 1.0, 1.0]).to(device)\n","\n","# CustomTrainer 인스턴스 생성 시 사용자 지정 콜백 추가\n","trainer_roberta = CustomTrainer(\n","    model=model_roberta,\n","    args=training_args,\n","    train_dataset=dataset_roberta,\n","    label_weights=label_weights,\n","    compute_metrics=compute_metrics,\n","    callbacks=[PrintProgressCallback()],  # 콜백 추가\n",")\n","\n","trainer_electra = CustomTrainer(\n","    model=model_electra,\n","    args=training_args,\n","    train_dataset=dataset_electra,\n","    label_weights=label_weights,\n","    compute_metrics=compute_metrics,\n","    callbacks=[PrintProgressCallback()],  # 콜백 추가\n",")\n","\n","# 훈련 시작\n","trainer_roberta.train()\n","trainer_electra.train()\n","\n","# 훈련된 RoBERTa 모델 저장\n","model_save_path = \"/home/ec2-user/to/save/roberta_model\"\n","trainer_roberta.save_model(model_save_path)\n","\n","# 훈련된 Electra 모델 저장\n","model_save_path = \"/home/ec2-user/to/save/electra_model\"\n","trainer_electra.save_model(model_save_path)"],"metadata":{"id":"BXRTr8l3AnWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global user.email 'osoryo@naver.com'\n","!git config --global user.name 'ChamsolPark'\n","\n","!git add Recommendation_Model/pre_train.ipynb\n","!git commit -m \"Add pre_train.ipynb to Recommendation_Model\"\n","\n","# 원격 저장소와 충돌 방지를 위해 먼저 pull\n","!git pull origin master\n","\n","# 변경 사항 push\n","!git push origin master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhBB0FxHAq_x","executionInfo":{"status":"ok","timestamp":1716042548956,"user_tz":-540,"elapsed":914,"user":{"displayName":"‍박참솔(학부생-소프트웨어전공)","userId":"18033728826949019778"}},"outputId":"f0c53f4f-7868-4488-c922-aca06ee433f9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WsTHpRw5HRPd"},"execution_count":null,"outputs":[]}]}